from selenium.webdriver.chrome.options import Options
from selenium.webdriver import Chrome
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import pandas as pd
import requests
import DataManager as dm

dates = []
ttl = []
desc = []
urll = []

chrome_options = Options()
chrome_options.add_argument('--headless=new')
driver = Chrome(options=chrome_options)

URL = 'https://www.pentestpartners.com/automotive-security/'
driver.get(URL)
source_name = "pentestpartners"
last_entry_date = dm.get_last_entry_date(source_name)
links = [my_elem.get_attribute("href") for my_elem in driver.find_elements(By.CSS_SELECTOR, '.archive-page a')]

for link in links:
    driver.get(link)
    title_element = driver.find_element(By.CLASS_NAME, 'page-heading')
    title = title_element.text
    date_element = driver.find_element(By.CLASS_NAME, 'date')
    date = date_element.text
    body_element = driver.find_element(By.CLASS_NAME, 'editor')
    if date != '' and pd.to_datetime(date) > last_entry_date:
        urll.append(link)
        ttl.append(title)
        dates.append(date)
for i in urll:
    req = requests.get(i)
    soup = BeautifulSoup(req.content, "html.parser")
    ref = soup.find('div', class_="editor")
    p_tags = ref.findAll('p')

    p_text = ""
    for p_tag in p_tags:
        if not p_tag.has_attr("class"):
            p_text += p_tag.text.strip("\n") + " "
    desc.append(p_text)
driver.quit()

record_flag = 0
descrip = ""
df = pd.DataFrame(list(zip(ttl, dates, desc, urll)), columns=['Title', 'Date', 'Body', 'URL'])
df['record_flag'] = record_flag
df['Description'] = descrip
df['Date'] = pd.to_datetime(df['Date'])
print(df.shape)
if df.empty:
    print("No new data to dump")
else:
    dm.insert_data(df)